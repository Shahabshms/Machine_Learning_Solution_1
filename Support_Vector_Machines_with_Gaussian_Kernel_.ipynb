{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machines with Gaussian Kernel .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNg8yZ7x9TpRGmUjGh60lJZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahabshms/Machine_Learning_Solution_1/blob/main/Support_Vector_Machines_with_Gaussian_Kernel_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR-fErK0K96D"
      },
      "source": [
        "# Problem 3: Support Vector Machines\n",
        "For this problem, consider the data set (mystery.data) attached to this homework that, like Problem 1, contains four numeric attributes per row and the fifth entry is the class variable (either $+$ or $-$). Find a perfect classifier for this data set using support vector machines. Your solution should explain the optimization problem that you solved and provide the learned parameters, the optimal margin, and the support vectors. Note, for full credit, your solution should only make use of a quadratic programming solver, i.e., you should not rely on existing SVM toolkits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9xwBG4JK1bC",
        "outputId": "9135d5c6-e82e-4443-e4f5-73a39d7a76c7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys, os\n",
        "\n",
        "col_names = ['x1','x2','x3','x4','y']\n",
        "!wget -nc https://personal.utdallas.edu/~shahab.shams/files/ML_Spring_2021/mystery.data\n",
        "data =  pd.read_csv('mystery.data', sep=\",\" , names = col_names)\n",
        "print(data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 17:19:09--  https://personal.utdallas.edu/~shahab.shams/files/ML_Spring_2021/mystery.data\n",
            "Resolving personal.utdallas.edu (personal.utdallas.edu)... 129.110.182.249\n",
            "Connecting to personal.utdallas.edu (personal.utdallas.edu)|129.110.182.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34159 (33K)\n",
            "Saving to: ‘mystery.data’\n",
            "\n",
            "mystery.data        100%[===================>]  33.36K   104KB/s    in 0.3s    \n",
            "\n",
            "2021-03-07 17:19:12 (104 KB/s) - ‘mystery.data’ saved [34159/34159]\n",
            "\n",
            "          x1        x2       x3       x4  y\n",
            "0    0.14343  0.258500  0.79126  0.53526  1\n",
            "1    0.19117  0.526940  0.28061  0.77913  1\n",
            "2    0.40839  0.239850  0.35756  0.80489  1\n",
            "3    0.16432  0.002530  0.40666  0.89868  1\n",
            "4    0.27962  0.177280  0.58566  0.73986  1\n",
            "..       ...       ...      ...      ... ..\n",
            "995  0.35432  0.235120  0.91744  0.98868 -1\n",
            "996  1.02650  0.756380  0.53262  0.30093 -1\n",
            "997  0.46145  0.751200  0.40619  1.02850 -1\n",
            "998  0.49309  0.076142  0.52661  1.21400 -1\n",
            "999  1.02920  0.682210  0.36240  0.58659 -1\n",
            "\n",
            "[1000 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I-dmCSjLSvr",
        "outputId": "aa0da02c-9c9a-41d5-82f3-761f1d476b9f"
      },
      "source": [
        "Y = data['y'].values\n",
        "Y = Y.reshape(-1,1)\n",
        "print('Y dimention: ',Y.shape)\n",
        "X = data[['x1','x2','x3','x4']].values\n",
        "print('X dimnetion: ',X.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y dimention:  (1000, 1)\n",
            "X dimnetion:  (1000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN5kG-HwMb9L"
      },
      "source": [
        "Primal Problem:\n",
        "$$\\min \\frac{1}{2}\\Vert w \\Vert_2^2$$\n",
        "such that\n",
        "$$\\forall i \\in \\{1,\\dots,M\\} \\quad\\quad  1 - y^{(i)}(w^T x^{(i)} + b) \\leq 0. $$\n",
        "To see why, please check the slides and lectures. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAfHbmOcLpq0"
      },
      "source": [
        "The promal problem is a quadradic obtimization and we can solve it. However this data set that we have is not linearly separable in its original domain.  To find the perfect classifier, one possible approch is to hire a modified feature vector like $[x_1^2,x_2^2,x_3^2,x_4^2, x_1x_2x_3x_4]$. But here we try to solve the dual problem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqEVDxHVZ9HR"
      },
      "source": [
        "Lagrangian:\n",
        "\n",
        "$$L(w,\\lambda)  = \\frac{1}{2}w^Tw + \\sum_{i=1}^M \\lambda_i (1-y^{(i)}(w^Tx^{(i)} + b))$$\n",
        "\n",
        "$$ \\frac{\\partial L(w,\\lambda)}{\\partial w} = w - \\sum_{i=1}^{M}\\lambda_i y^{(i)}x^{(i)} = 0 \\rightarrow w = \\sum_{i=1}^{M}\\lambda_iy^{(i)}x^{(i)}$$\n",
        "$$ \\frac{\\partial L(w,\\lambda)}{\\partial b} =  -\\sum_{i=1}^{M}\\lambda_iy^{(i)} = 0 \\rightarrow \\sum_{i=1}^{M}\\lambda_iy^{(i)} = 0 $$\n",
        "\n",
        "Now by substituting $w$ back in the Lagrangian we get the dual problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMNQdLp_dQ10"
      },
      "source": [
        "Dual Problem\n",
        "\n",
        "$$\\max_{\\lambda} -\\frac{1}{2}\\sum_{i=1}^{M}\\sum_{j=1}^{M} \\lambda_i\\lambda_jy^{(i)}y^{(j)}\\langle x^{(i)},x^{(j)}\\rangle + \\sum_{i=1}^{M}\\lambda_i$$\n",
        "such that\n",
        "$$\\sum_{i=1}^{M} \\lambda_iy^{(i)} = 0$$\n",
        "$$\\lambda_i\\geq 0 \\quad \\forall i\\in\\{1,\\dots,M\\}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av6R8j-FCyVl"
      },
      "source": [
        "The result of Dual problem is the Lagrange multipliers, $\\lambda = [\\lambda_1,\\dots,\\lambda_M]$. These variables show us which data points are support vectors. \n",
        "$$\\forall i \\in \\{1,\\dots,M\\} \\text{ if } \\lambda_i > 0 \\text{ then } x^{(i)} \\text{ is a support vector.}$$\n",
        "To see why, please check the lectures and slides, or come during office hours. \n",
        "\n",
        "In the following, we will see how to use support vectors to get a classifier. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7J26NLspGGv"
      },
      "source": [
        "Kernel Trick for Dual SVM\n",
        "\n",
        "$$\\max_{\\lambda} -\\frac{1}{2}\\sum_{i=1}^M\\sum_{j=1}^M \\lambda_i\\lambda_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)}) + \\sum_{i=1}^M \\lambda_i$$\n",
        "The Kernel function can be anything such as Gaussian, $K(x^{(i)},x^{(j)}) = \\exp(-\\gamma\\Vert x^{(i)} - x^{(j)}\\Vert^2)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KkFvEpyOPTA"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def poly_func(X,poly_case):\n",
        "  if poly_case == 'Full-2':\n",
        "    return np.array([X[0],X[1],X[2],X[3],\n",
        "                     X[0]**2,X[1]**2,X[2]**2,X[3]**2,\n",
        "                     X[0]*X[1],X[1]*X[2],X[2]*X[3],X[0]*X[2],X[1]*X[3],X[0]*X[3],\n",
        "                     X[0]*X[1]*X[2]*X[3]])\n",
        "  elif poly_case == 5:\n",
        "    return np.array([X[0]**2,X[1]**2,X[2]**2,X[3]**2,X[0]*X[1]*X[2]*X[3]])\n",
        "\n",
        "  elif poly_case == 'Linear':\n",
        "    return X\n",
        "\n",
        "def K(X1,X2,kernel_option,gamma = 1,poly_case = 'Linear'):\n",
        "  if kernel_option == 'Gaussian':\n",
        "    return  np.exp(-gamma * np.linalg.norm(X1 - X2)**2)/2\n",
        "  if kernel_option == 'Polynomial':\n",
        "    v1 = poly_func(X1,poly_case)\n",
        "    v2 = poly_func(X2,poly_case)\n",
        "    return v1.dot(v2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7SDDtqnXUJ_"
      },
      "source": [
        "Solving Quadratic Programming with CVXOPT\n",
        "\n",
        "$$\\min \\frac{1}{2}x^T P x + q^Tx$$\n",
        "such that \n",
        "$$Gx \\preceq h$$\n",
        "$$Ax = b$$\n",
        "\n",
        "See the Quadratic Programming\n",
        " from https://cvxopt.org/userguide/coneprog.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1BqZSu3YRWF"
      },
      "source": [
        "Here, we do not implement the optimization process from the scratch. Instead we utilize CVXOPT. In order to use CVXOPT's API, we have to convert our Dual Problem into a proper notation that is undrestandable for CVXOPT. So, the followin step is just creating new matrices with proper names and then simply passing them to CVXOPT.\n",
        "Just notice that our variable is $\\lambda$ and not $x$. Both $y$ and $x$ are given through the data set, and it's only Lagranle multipliers that we have to find.\n",
        "\n",
        "The translation into CVXOPT is as follow.\n",
        "\n",
        "Given $Y\\in\\mathbb{R}^{M\\times 1}$ and $X\\in\\mathbb{R}^{M\\times 4}$\n",
        "0. Convert the maximization to minimization by multiplying the objectiv function to -1.  \n",
        "1. $P = YY^T\\odot K(X,X)$      \n",
        "when $\\odot$ is the Hadamard product, and matrix $K(X,X)\\in\\mathbb{R}^{M\\times M}$ such that $\\forall i,j\\in\\{1,\\dots,M\\}$, $K(X,X)_{i,j} = K(X_i,X_j)$. \n",
        "2. $q = [-1]^M$\n",
        "3. $G = -I_{M\\times M}$\n",
        "4. $h = [0]^M$\n",
        "5. $A = Y^T$\n",
        "6. $b = 0$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eee9Nz3Je39T"
      },
      "source": [
        "from cvxopt import matrix as cvxopt_matrix\n",
        "from cvxopt import solvers\n",
        "\n",
        "def SVM_with_Kernel(X,Y,kernel_option, gamma = 1,poly_case = 'Linear'):\n",
        "  m,n = X.shape\n",
        "  H = np.zeros([m,m])\n",
        "  for i in range(m):\n",
        "    for j in range(m):\n",
        "      H[i,j] = Y[i]*Y[j] * K(X[i],X[j], kernel_option,gamma, poly_case)\n",
        "\n",
        "#Converting into cvxopt format\n",
        "  P = cvxopt_matrix(H)\n",
        "  q = cvxopt_matrix(-np.ones((m, 1)))\n",
        "  G = cvxopt_matrix(-np.eye(m))\n",
        "  h = cvxopt_matrix(np.zeros(m))\n",
        "  A = cvxopt_matrix(Y.reshape(1, -1)* 1.)\n",
        "  b = cvxopt_matrix(np.zeros(1))\n",
        "#Run solver\n",
        "  solvers.options['show_progress'] = False\n",
        "  sol = solvers.qp(P, q, G, h, A, b);\n",
        "  lambdas = np.array(sol['x']);\n",
        "\n",
        "  return lambdas"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vVrysqAg3y0"
      },
      "source": [
        "What happen after we have found Lagrange multipliers? \n",
        "\n",
        "Looking back at the procedure that converted the Primal problem into the Dual problem, we find \n",
        "$$w = \\sum_{i=1}^{M}\\lambda_iy^{(i)}x^{(i)}.$$\n",
        "$y$ and $x$ are given. And now that we have $\\lambda$, we can simply find $w$. For finding the intercept, $b$, we did not encouter any specific formula. But, since we have the support vectors, we can calculate $b$ as well. Remmember that we started by forcing\n",
        "$$\\forall i \\in \\{1,\\dots,M\\} \\quad\\quad  1 - y^{(i)}(w^T x^{(i)} + b) \\leq 0. $$\n",
        "We know that for support vectors, $1 - y^{(i)}(w^T x^{(i)} + b) = 0$. So, for each support vectors as $s$, we have $$b = \\frac{1 - y^{(s)}w^Tx^{(s)}}{y^{(s)}} \\rightarrow \\begin{cases} b = 1 - w^Tx^{(s)}& y^{(s)} = 1 \\\\ b = -1 - w^Tx{(s)}& y^{(s)} = -1\\end{cases}$$\n",
        "So,\n",
        "$$b = y^{(s)} - w^Tx^{(s)}.$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiaZUq6_SIaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0315b879-44c8-4d87-aa1a-e14302a045a4"
      },
      "source": [
        "kernel_option = 'Polynomial'\n",
        "# 4 is the Linear\n",
        "\n",
        "dims = {'Linear':4,5:5,'Full-2':15}\n",
        "\n",
        "for poly_case in ['Linear',5,'Full-2']:\n",
        "  print('\\nKernel Option: ',kernel_option,poly_case)\n",
        "  lambdas = SVM_with_Kernel(X,Y,kernel_option,poly_case = poly_case)\n",
        "\n",
        "  modified_X = np.zeros([Y.shape[0],dims[poly_case]])\n",
        "  for n in range(Y.shape[0]):\n",
        "    modified_X[n] = poly_func(X[n],poly_case)\n",
        "\n",
        "  w = ((Y * lambdas).T @ modified_X).reshape(-1,1);\n",
        "  S = (lambdas > 1e-3).flatten();\n",
        "  b = Y[S] - np.dot(modified_X[S], w);\n",
        "  # print('lambdas = ',lambdas[lambdas > 1e-4])\n",
        "  Y_pred = modified_X.dot(w) + b[0]\n",
        "  print('w = ', w.flatten())\n",
        "  print('b = ', b[0])\n",
        "  res = sum([int(Y[i] == np.sign(Y_pred[i])) for i in range(Y.shape[0])])\n",
        "  print('Accuracy:', res)\n",
        "  if poly_case == 'Linear':continue\n",
        "  print('Support Vectors: \\n', np.where(lambdas>0.001)[0])\n",
        "  print(Y[np.where(lambdas>0.001)[0]].T[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Kernel Option:  Polynomial Linear\n",
            "w =  [-0.00305176 -0.00536728 -0.00656128 -0.00317383]\n",
            "b =  [1.00871566]\n",
            "Accuracy: 500\n",
            "\n",
            "Kernel Option:  Polynomial 5\n",
            "w =  [-2.00026926e+00 -2.00029657e+00 -2.00027860e+00 -2.00028939e+00\n",
            " -1.01471662e-04]\n",
            "b =  [3.00031234]\n",
            "Accuracy: 1000\n",
            "Support Vectors: \n",
            " [  9  64  94 139 144 190 198 213 277 358 411 413 450 468 484 643 645 724\n",
            " 735 810 888 891]\n",
            "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1]\n",
            "\n",
            "Kernel Option:  Polynomial Full-2\n",
            "w =  [-0.28567985 -0.29206962 -0.25582616 -0.33389673 -1.94176315 -1.94520011\n",
            " -1.9568138  -1.92587606  0.12357456  0.11490359  0.12791295  0.11128354\n",
            "  0.15288799  0.14159597 -0.04773691]\n",
            "b =  [3.3360581]\n",
            "Accuracy: 1000\n",
            "Support Vectors: \n",
            " [139 328 645 700 746 816 817 833 852 860 879 960]\n",
            "[ 1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPTDaCIWf9oY"
      },
      "source": [
        "For the Gaussian Kernel, predicting is slightly different. Remmember how we get to use the Kernel function.\n",
        "$$\\quad\\quad\\Phi(x^{(i)})^T\\Phi(x^{(j)}) \\equiv K(x^{(i)},x^{(j)}) $$\n",
        "We start by creating the modified version of our data set, applying $\\Phi(x)$ to every data point. Furthur, we replace $\\Phi(x^{(i)})^T\\Phi(x^{(j)})$ with $K(x^{(i)},x^{(j)})$ for simplicity. However, this trick allows us to use Kernel functions that we do not necessarily have a feature vector corresponding to. The Gaussian Kernel is one of those Kernel functions that requires a feature vector of lenght infinity. You may think this makes Gaussian Kernel useless. But actually this is why Gaussian Kernel is the most popular one. The main advantage of the Gaussian Kernel is that, with a very low cost, it takes your data set into an inifinite dimention and you do the classification there. But in the case of SVM, it only results in the support vectors, based on which you can not immediately calculate a separating line. The separating need to be done based on the similarities that each query point has with the support vectors. This task requires the influence that each support vector has in the separating process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufM922iec35X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8eaa3a-5ba5-4c61-88b5-e13aa47c0b07"
      },
      "source": [
        "lambdas = SVM_with_Kernel(X,Y,'Gaussian', gamma = 1/ (X.var() * X.shape[1]))\n",
        "supports = X[np.where(lambdas > 1)[0]]\n",
        "print('Indices of support vectors:\\n',np.where(lambdas > 1)[0])\n",
        "print('Support Vectors:\\n',supports)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indices of support vectors:\n",
            " [  7  45 134 162 191 224 261 265 342 358 464 591 628 650 700 746 776 817\n",
            " 833 850 860 879 939 979 994]\n",
            "Support Vectors:\n",
            " [[0.0043834 0.60143   0.78558   0.14537  ]\n",
            " [0.65996   0.035779  0.7408    0.11994  ]\n",
            " [0.28969   0.19852   0.93181   0.091634 ]\n",
            " [0.95018   0.084484  0.016428  0.29957  ]\n",
            " [0.057479  0.0093453 0.83091   0.55335  ]\n",
            " [0.84726   0.038806  0.52722   0.051884 ]\n",
            " [0.99068   0.054552  0.085061  0.091362 ]\n",
            " [0.059681  0.96868   0.065506  0.23196  ]\n",
            " [0.63954   0.017487  0.036738  0.76768  ]\n",
            " [0.60991   0.7887    0.06143   0.046915 ]\n",
            " [0.035301  0.07096   0.022447  0.9966   ]\n",
            " [0.038325  0.0025055 1.0011    0.99821  ]\n",
            " [0.024464  0.056681  0.20296   1.3982   ]\n",
            " [0.041857  1.1835    0.079195  0.76902  ]\n",
            " [0.1363    0.022961  0.01358   1.4074   ]\n",
            " [1.2888    0.02535   0.021734  0.58119  ]\n",
            " [0.87856   1.0758    0.098352  0.24741  ]\n",
            " [0.2554    0.43854   1.3177    0.078137 ]\n",
            " [1.356     0.30684   0.13052   0.22375  ]\n",
            " [0.70627   0.042314  0.032845  1.2241   ]\n",
            " [0.10956   1.3847    0.035443  0.26341  ]\n",
            " [0.21853   1.3895    0.1212    0.082701 ]\n",
            " [1.2586    0.055016  0.59157   0.25073  ]\n",
            " [1.0523    0.19523   0.9091    0.16734  ]\n",
            " [0.094267  0.0063349 1.1528    0.81377  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs0YWAU2qt3V"
      },
      "source": [
        "similarities = np.zeros([Y.shape[0],supports.shape[0]])\n",
        "for m in range(Y.shape[0]):\n",
        "  for s in range(supports.shape[0]):\n",
        "    similarities[m,s] = K(X[m],supports[s],kernel_option='Gaussian',gamma=1/ (X.var() * X.shape[1]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLsMOgtBwd2T",
        "outputId": "f2ba7a57-a01d-4d67-9b7a-fd163e07031f"
      },
      "source": [
        "lambdas = SVM_with_Kernel(similarities,Y,'Polynomial',poly_case='Linear')\n",
        "\n",
        "w = ((Y * lambdas).T @ similarities).reshape(-1,1);\n",
        "S = (lambdas > 1e-3).flatten();\n",
        "b = Y[S] - np.dot(similarities[S], w);\n",
        "Y_pred = similarities.dot(w) + b[0]\n",
        "res = sum([int(Y[i] == np.sign(Y_pred[i])) for i in range(Y.shape[0])])\n",
        "print('Accuracy:', res)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QzW9qvtwIo3"
      },
      "source": [
        "Compare with sklearn.svm\n",
        "\n",
        "This is not a part of your homework, but it is good to have an alternative method and check the result. \n",
        "\n",
        "Please check the support vectors that we have found using the Gaussian Kernel, and compare them with those that sklearn has found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcQOHzJPvF4M",
        "outputId": "cd77b8b5-2290-4834-9338-38a0ff0796b9"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "print('\\nKernel: Radial Basis')\n",
        "clf = SVC(kernel = 'rbf')\n",
        "clf.fit(X, Y.ravel()) \n",
        "Y_pred = clf.predict(X).reshape(-1,1)\n",
        "print('accuracy: ',np.sum(Y_pred == Y))\n",
        "print('Indices of support vectors = \\n', np.sort(clf.support_))\n",
        "# print('Support vectors = \\n', clf.support_vectors_)\n",
        "print('Number of support vectors for each class = ', clf.n_support_)\n",
        "\n",
        "print('\\nKernel: Polynomial')\n",
        "clf = SVC(kernel = 'poly')\n",
        "clf.fit(X, Y.ravel()) \n",
        "Y_pred = clf.predict(X).reshape(-1,1)\n",
        "print('accuracy: ',np.sum(Y_pred == Y))\n",
        "print('Indices of support vectors = \\n', np.sort(clf.support_))\n",
        "# print('Support vectors = \\n', clf.support_vectors_)\n",
        "print('Number of support vectors for each class = ', clf.n_support_)\n",
        "\n",
        "print('\\nKernel: Linear')\n",
        "clf = SVC(kernel = 'linear')\n",
        "clf.fit(X, Y.ravel()) \n",
        "Y_pred = clf.predict(X).reshape(-1,1)\n",
        "print('accuracy: ',np.sum(Y_pred == Y))\n",
        "print('w: ',clf.coef_)\n",
        "print('b: ',clf.intercept_)\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Kernel: Radial Basis\n",
            "accuracy:  1000\n",
            "Indices of support vectors = \n",
            " [  7  45  81  84  93 134 147 162 169 191 224 247 259 261 265 270 279 315\n",
            " 342 343 358 389 409 443 464 491 501 503 560 566 591 593 628 631 641 643\n",
            " 645 650 659 664 676 700 709 734 746 776 779 789 814 816 817 822 829 833\n",
            " 850 852 860 879 880 882 906 922 939 979 990 994]\n",
            "Number of support vectors for each class =  [40 26]\n",
            "\n",
            "Kernel: Polynomial\n",
            "accuracy:  1000\n",
            "Indices of support vectors = \n",
            " [ 56  74  78 117 118 234 253 360 370 463 501 566 591 643 645 700 709 746\n",
            " 814 817 850 879 979]\n",
            "Number of support vectors for each class =  [13 10]\n",
            "\n",
            "Kernel: Linear\n",
            "accuracy:  977\n",
            "w:  [[-4.58962092 -4.70044817 -4.3506897  -6.10745074]]\n",
            "b:  [10.41611276]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}